{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b63b7384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "\n",
    "path = 'OpenGVLab/InternVL3-1B'\n",
    "from minigemini.processor import Mini1oProcessor, Mini1oImageProcessor\n",
    "\n",
    "image_processor = Mini1oImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "chat_template = \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}\\n{% if content['type'] == 'image_gen' or 'image_gen' in content %}<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\\n{% elif content['type'] == 'video_gen' or 'video_gen' in content %}<|video_gen_start|><|video_gen_pad|><|video_gen_end|>\\n{% elif content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>\\n{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\\n\"\n",
    "\n",
    "processor1o = Mini1oProcessor(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e8d3523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor1o.save_pretrained('kirp/mini1o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ed587f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e077d960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original \n",
    "pixel_values = load_image('1.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "question = '<image>\\nPlease describe the image shortly.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "443f09ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          ...,\n",
       "          [ 2.2031,  2.2031,  2.2031,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.1875,  2.1875,  2.1875,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.1875,  2.2031,  2.2031,  ...,  2.2500,  2.2500,  2.2500]],\n",
       "\n",
       "         [[ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          ...,\n",
       "          [ 2.3750,  2.3906,  2.3906,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.3750,  2.3750,  2.3750,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.3750,  2.3750,  2.3750,  ...,  2.4219,  2.4219,  2.4219]],\n",
       "\n",
       "         [[ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          ...,\n",
       "          [ 2.6250,  2.6250,  2.6250,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6094,  2.6094,  2.6094,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6250,  2.6250,  2.6250,  ...,  2.6406,  2.6406,  2.6406]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          ...,\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  0.1426,  2.2500,  2.2344],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ..., -0.0459,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ..., -0.0287,  2.2500,  2.2344]],\n",
       "\n",
       "         [[ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          ...,\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  1.7109,  2.4219,  2.3906],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  1.6406,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  1.6406,  2.4219,  2.3906]],\n",
       "\n",
       "         [[ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          ...,\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.5156,  2.6406,  2.3906],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.4062,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.4375,  2.6406,  2.6250]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          ...,\n",
       "          [ 0.7422,  0.0569,  1.0703,  ...,  2.1406,  1.1562,  1.0000],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ..., -0.5430, -1.6250, -1.6719],\n",
       "          [ 2.1562,  2.2500,  2.2500,  ..., -2.0625, -2.1250, -2.0938]],\n",
       "\n",
       "         [[ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          ...,\n",
       "          [ 1.2578,  1.3594,  1.7969,  ...,  2.3125,  1.3047,  1.1719],\n",
       "          [ 2.4062,  2.4219,  2.4219,  ..., -0.3906, -1.4922, -1.5625],\n",
       "          [ 2.4062,  2.4219,  2.4219,  ..., -1.9453, -2.0000, -1.9844]],\n",
       "\n",
       "         [[ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          ...,\n",
       "          [ 1.9219,  2.1562,  2.3125,  ...,  2.5312,  1.6641,  1.5234],\n",
       "          [ 2.5625,  2.6406,  2.6406,  ...,  0.1650, -0.7930, -0.8477],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ..., -1.2109, -1.2266, -1.2266]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-1.4688,  1.0703,  2.2500,  ..., -2.0625, -2.0312, -2.0312],\n",
       "          [-1.6562,  0.1084,  2.2500,  ..., -2.0469, -2.0312, -2.0312],\n",
       "          [-0.8008, -0.9883,  1.4062,  ..., -2.0469, -2.0312, -2.0312],\n",
       "          ...,\n",
       "          [ 2.2500,  2.2500,  1.5469,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500, -0.3535,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  1.1719, -1.5703,  ...,  2.2500,  2.2500,  2.2500]],\n",
       "\n",
       "         [[ 0.5898,  2.1250,  2.4219,  ..., -1.9453, -1.9141, -1.9141],\n",
       "          [ 0.3457,  1.6953,  2.4219,  ..., -1.9297, -1.9141, -1.9141],\n",
       "          [ 0.4336,  1.1016,  1.9922,  ..., -1.9297, -1.9141, -1.9141],\n",
       "          ...,\n",
       "          [ 2.4219,  2.4219,  1.5156,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.1094,  0.6445,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  1.3984,  0.5742,  ...,  2.4219,  2.4219,  2.4219]],\n",
       "\n",
       "         [[ 1.8203,  2.6094,  2.6406,  ..., -1.1953, -1.1562, -1.1562],\n",
       "          [ 1.6641,  2.4062,  2.6406,  ..., -1.1797, -1.1562, -1.1562],\n",
       "          [ 1.6094,  2.0938,  2.3906,  ..., -1.1797, -1.1562, -1.1562],\n",
       "          ...,\n",
       "          [ 2.6406,  2.5625,  1.9062,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.3125,  1.6094,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.5469,  1.8047,  1.8594,  ...,  2.6406,  2.6406,  2.6406]]],\n",
       "\n",
       "\n",
       "        [[[-2.0625, -2.0625,  0.8945,  ...,  2.1875,  2.1875,  2.1875],\n",
       "          [-2.0312, -2.1250,  0.7422,  ...,  2.2031,  2.2031,  2.2031],\n",
       "          [-2.0312, -2.1250,  0.7773,  ...,  2.2031,  2.2031,  2.2031],\n",
       "          ...,\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2188,  2.2188,  2.2188],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2188,  2.2188,  2.2188],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2188,  2.2188,  2.2031]],\n",
       "\n",
       "         [[-1.9453, -1.9453,  1.0469,  ...,  2.3750,  2.3750,  2.3750],\n",
       "          [-1.9141, -2.0312,  0.8867,  ...,  2.3906,  2.3906,  2.3750],\n",
       "          [-1.9141, -2.0312,  0.9219,  ...,  2.3906,  2.3906,  2.3750],\n",
       "          ...,\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.3906,  2.3906,  2.3906],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4062,  2.4062,  2.3906],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.3906,  2.3906,  2.3906]],\n",
       "\n",
       "         [[-1.1797, -1.1953,  1.4375,  ...,  2.6250,  2.6250,  2.6094],\n",
       "          [-1.1562, -1.3516,  1.3125,  ...,  2.6250,  2.6250,  2.6250],\n",
       "          [-1.1562, -1.3203,  1.3125,  ...,  2.6250,  2.6250,  2.6250],\n",
       "          ...,\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6250,  2.6250,  2.6250],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6250],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6250,  2.6250,  2.6250]]],\n",
       "\n",
       "\n",
       "        [[[ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          [ 2.2500,  2.2500,  2.2500,  ...,  2.2500,  2.2500,  2.2500],\n",
       "          ...,\n",
       "          [ 2.2188,  2.2188,  2.2188,  ...,  2.2188,  2.2188,  2.2188],\n",
       "          [ 2.2188,  2.2188,  2.2188,  ...,  2.2188,  2.2188,  2.2188],\n",
       "          [ 2.2188,  2.2188,  2.2188,  ...,  2.2188,  2.2188,  2.2188]],\n",
       "\n",
       "         [[ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          [ 2.4219,  2.4219,  2.4219,  ...,  2.4219,  2.4219,  2.4219],\n",
       "          ...,\n",
       "          [ 2.3906,  2.3906,  2.3906,  ...,  2.3906,  2.3906,  2.3906],\n",
       "          [ 2.3906,  2.4062,  2.3906,  ...,  2.3906,  2.4062,  2.3906],\n",
       "          [ 2.3906,  2.3906,  2.3906,  ...,  2.3906,  2.3906,  2.3906]],\n",
       "\n",
       "         [[ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          [ 2.6406,  2.6406,  2.6406,  ...,  2.6406,  2.6406,  2.6406],\n",
       "          ...,\n",
       "          [ 2.6250,  2.6250,  2.6250,  ...,  2.6250,  2.6250,  2.6250],\n",
       "          [ 2.6250,  2.6250,  2.6250,  ...,  2.6250,  2.6250,  2.6250],\n",
       "          [ 2.6250,  2.6250,  2.6250,  ...,  2.6250,  2.6250,  2.6250]]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00856f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>\n",
      "Please describe the image shortly.<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "\n",
    "path = 'OpenGVLab/InternVL3-1B'\n",
    "from minigemini.processor import Mini1oProcessor, Mini1oImageProcessor\n",
    "\n",
    "image_processor = Mini1oImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "chat_template = \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}\\n{% if content['type'] == 'image_gen' or 'image_gen' in content %}<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\\n{% elif content['type'] == 'video_gen' or 'video_gen' in content %}<|video_gen_start|><|video_gen_pad|><|video_gen_end|>\\n{% elif content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>\\n{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\\n\"\n",
    "tokenizer.chat_template = chat_template\n",
    "processor1o = Mini1oProcessor(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n",
    "\n",
    "processor1o.save_pretrained('kirp/mini1o')\n",
    "image_processor.save_pretrained('kirp/mini1o')\n",
    "tokenizer.save_pretrained('kirp/mini1o')\n",
    "\n",
    "## new\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": Image.open('1.png').convert('RGB'),\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Please describe the image shortly.\"\n",
    "            },\n",
    "            {\n",
    "                'type': 'image_gen',\n",
    "                'image_gen': Image.open('1.png').convert('RGB'),\n",
    "            }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor1o.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e3b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[Image.open('1.png').convert('RGB')],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aff221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigemini.processor import Mini1oImageProcessor\n",
    "from PIL import Image\n",
    "image_processor = Mini1oImageProcessor()\n",
    "image_processor.save_pretrained('kirp/mini1o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ee5025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Image.open('1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce97d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_transforms import (\n",
    "    convert_to_rgb,\n",
    "    resize\n",
    ")\n",
    "x = convert_to_rgb(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5d83ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = image_processor.dynamic_preprocess(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "528d093f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(448, 448, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array(x[0])\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "939841d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.uint8(0),\n",
       " np.uint8(255),\n",
       " np.float64(244.9112474091199),\n",
       " np.float64(28.72002901373138))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min(), y.max(), y.mean(), y.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6cc620b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = processor1o(\n",
    "    text=[text],\n",
    "    images=[Image.open('1.png').convert('RGB')],\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "193237bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>\n",
      "Please describe the image shortly.<|image_gen_start|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_pad|><|image_gen_end|>\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = processor1o.batch_decode(x['input_ids'], skip_special_tokens=False)[0]\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1690216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3, 448, 448])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42d0f314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('kirp/mini1o\\\\tokenizer_config.json',\n",
       " 'kirp/mini1o\\\\special_tokens_map.json',\n",
       " 'kirp/mini1o\\\\vocab.json',\n",
       " 'kirp/mini1o\\\\merges.txt',\n",
       " 'kirp/mini1o\\\\added_tokens.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor1o.save_pretrained('kirp/mini1o')\n",
    "tokenizer.save_pretrained('kirp/mini1o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cb8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?\n",
      "Assistant: Hello! I'm an AI assistant here to help you with information and answering questions. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from transformers import AutoConfig\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values\n",
    "# If you set `load_in_8bit=True`, you will need two 80GB GPUs.\n",
    "# If you set `load_in_8bit=False`, you will need at least three 80GB GPUs.\n",
    "# path = 'OpenGVLab/InternVL3-1B'\n",
    "path = \"OpenGVLab/InternVL3-1B\"\n",
    "\n",
    "# device_map = split_model(path)\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    load_in_8bit=False,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto').eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "\n",
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('1.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# question = 'Can you tell me a story?'\n",
    "# response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # single-image single-round conversation (单图单轮对话)\n",
    "# question = '<image>\\nPlease describe the image shortly.'\n",
    "# response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # single-image multi-round conversation (单图多轮对话)\n",
    "# question = '<image>\\nPlease describe the image in detail.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# question = 'Please write a poem according to the image.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # multi-image multi-round conversation, combined images (多图多轮对话，拼接图像)\n",
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "# question = '<image>\\nDescribe the two images in detail.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                history=None, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# question = 'What are the similarities and differences between these two images.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                history=history, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # multi-image multi-round conversation, separate images (多图多轮对话，独立图像)\n",
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "# num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "# question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list,\n",
    "#                                history=None, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# question = 'What are the similarities and differences between these two images.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list,\n",
    "#                                history=history, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # batch inference, single image per sample (单图批处理)\n",
    "# pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "# num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "# pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "# questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "# responses = model.batch_chat(tokenizer, pixel_values,\n",
    "#                              num_patches_list=num_patches_list,\n",
    "#                              questions=questions,\n",
    "#                              generation_config=generation_config)\n",
    "# for question, response in zip(questions, responses):\n",
    "#     print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# # video multi-round conversation (视频多轮对话)\n",
    "# def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "#     if bound:\n",
    "#         start, end = bound[0], bound[1]\n",
    "#     else:\n",
    "#         start, end = -100000, 100000\n",
    "#     start_idx = max(first_idx, round(start * fps))\n",
    "#     end_idx = min(round(end * fps), max_frame)\n",
    "#     seg_size = float(end_idx - start_idx) / num_segments\n",
    "#     frame_indices = np.array([\n",
    "#         int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "#         for idx in range(num_segments)\n",
    "#     ])\n",
    "#     return frame_indices\n",
    "\n",
    "# def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "#     vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "#     max_frame = len(vr) - 1\n",
    "#     fps = float(vr.get_avg_fps())\n",
    "\n",
    "#     pixel_values_list, num_patches_list = [], []\n",
    "#     transform = build_transform(input_size=input_size)\n",
    "#     frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "#     for frame_index in frame_indices:\n",
    "#         img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "#         img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "#         pixel_values = [transform(tile) for tile in img]\n",
    "#         pixel_values = torch.stack(pixel_values)\n",
    "#         num_patches_list.append(pixel_values.shape[0])\n",
    "#         pixel_values_list.append(pixel_values)\n",
    "#     pixel_values = torch.cat(pixel_values_list)\n",
    "#     return pixel_values, num_patches_list\n",
    "\n",
    "# video_path = './examples/red-panda.mp4'\n",
    "# pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "# pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "# video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "# question = video_prefix + 'What is the red panda doing?'\n",
    "# # Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# question = 'Describe this video in detail.'\n",
    "# response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "#                                num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "# print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6507f535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 3, 448, 448])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8016c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x = model.extract_feature(pixel_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bfab3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([9, 256, 896])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adc40a90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image is a contract listing on a cryptocurrency trading website. Here's a description:\n",
      "\n",
      "- **Contract Address**: 0x34C621162f12763c6e0Eb007dc2aE91090A2d22f6.\n",
      "- **Sponsorship Promotion**: This token is reported to be a honeyypot token.\n",
      "- **Overview**:\n",
      "  - Ether balance is $0.00.\n",
      "  - Ether value is $0.00.\n",
      "  - Token holdings are $0.00 (1 token).\n",
      "  - No contract holder addresses are found.\n",
      "- **More Info**:\n",
      "  - There is a private name tag for the contract holder.\n",
      "  - Contract creator is labeled as BELL ELE Honeypot Rug P.\n",
      "  - ERC-20 token balance is displayed as $0 (for the multichain portfolio).\n",
      "- **Recent Transactions**:\n",
      "  - Recent transactions include approvals on specific blocks, with the first block transaction being approved on October 215,194,400 days ago.\n",
      "- **Download Page Data**: A download button is visible for the page data. \n",
      "\n",
      "The interface includes options to buy, exchange, play, and gaming.\n"
     ]
    }
   ],
   "source": [
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20d2d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
