{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f175977",
   "metadata": {},
   "source": [
    "# config prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aee366b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mini1o import DitConfig\n",
    "# import torch\n",
    "# from diffusers import SanaTransformer2DModel, AutoencoderDC, DPMSolverMultistepScheduler\n",
    "# model = SanaTransformer2DModel.from_pretrained(\"Efficient-Large-Model/Sana_600M_512px_diffusers\",subfolder=\"transformer\",torch_dtype=torch.bfloat16)\n",
    "# # 加载 VAE 模型，用于将图像编码到 latent 空间\n",
    "# vae = AutoencoderDC.from_pretrained(\"Efficient-Large-Model/Sana_600M_512px_diffusers\", subfolder=\"vae\",torch_dtype=torch.bfloat16)\n",
    "# # 加载 scheduler，该组件封装了向 latent 添加噪声的操作及时间步信息\n",
    "# scheduler = DPMSolverMultistepScheduler.from_pretrained(\"Efficient-Large-Model/Sana_600M_512px_diffusers\",subfolder=\"scheduler\",torch_dtype=torch.bfloat16,)\n",
    "# # 从 scheduler 配置中获取训练时的总时间步数（如果配置中没有该项，可设定一个默认值，如 1000）\n",
    "# model_config = dict(model.config)\n",
    "# vae_config = dict(vae.config)\n",
    "# scheduler_config = dict(scheduler.config)\n",
    "# dit_config = DitConfig(\n",
    "#     model_config=model_config,\n",
    "#     vae_config=vae_config,\n",
    "#     scheduler_config=scheduler_config\n",
    "# )\n",
    "from transformers import AutoConfig\n",
    "path = \"OpenGVLab/InternVL3-1B\"\n",
    "# path = 'OpenGVLab/InternVL3-1B'\n",
    "\n",
    "# path = ''\n",
    "# mllm_config = AutoConfig.from_pretrained(path, trust_remote_code=True)\n",
    "from mini1o import Mini1oMLLM,Mini1oConfig, Mini1o\n",
    "config = Mini1oConfig()\n",
    "config.save_pretrained('ckpt', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ace2f3",
   "metadata": {},
   "source": [
    "# processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c4f286d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加入特殊token\n",
    "from transformers import AutoTokenizer, AutoProcessor\n",
    "\n",
    "\n",
    "# 加载 tokenizer 和 processor\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True)\n",
    "processor = AutoProcessor.from_pretrained(path, trust_remote_code=True)\n",
    "\n",
    "# 定义你要加的 special tokens\n",
    "special_tokens = [\n",
    "    \"<|image_gen_start|>\", \"<|image_gen_pad|>\", \"<|image_gen_end|>\",\n",
    "    # \"<|video_gen_start|>\", \"<|video_gen_pad|>\", \"<|video_gen_end|>\"\n",
    "]\n",
    "\n",
    "# 添加 token，并获得它们的 ID\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens+tokenizer.additional_special_tokens})\n",
    "processor.tokenizer = tokenizer  # 更新 processor 的 tokenizer\n",
    "\n",
    "# 映射到 ID\n",
    "token_ids = {token: tokenizer.convert_tokens_to_ids(token) for token in special_tokens}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ffee1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"OpenGVLab/InternVL3-1B\"\n",
    "from mini1o.processor import Mini1oProcessor, Mini1oImageProcessor\n",
    "from transformers import AutoTokenizer\n",
    "from diffusers.image_processor import PixArtImageProcessor\n",
    "\n",
    "image_processor = Mini1oImageProcessor()\n",
    "gen_image_processor = PixArtImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "chat_template = \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}\\n{% if content['type'] == 'image_gen' or 'image_gen' in content %}<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\\n{% elif content['type'] == 'video_gen' or 'video_gen' in content %}<|video_gen_start|><|video_gen_pad|><|video_gen_end|>\\n{% elif content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>\\n{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\\n\"\n",
    "\n",
    "processor1o = Mini1oProcessor(image_processor=image_processor, \n",
    "                              tokenizer=tokenizer, \n",
    "                              chat_template=chat_template)\n",
    "# processor1o.save_pretrained('ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>\n",
      "Please describe the image shortly.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": Image.open('1.png').convert('RGB'),\n",
    "            },\n",
    "            {\n",
    "                \"text\": \"Please describe the image shortly.\\n\"\n",
    "            },\n",
    "            # {\n",
    "            #     'image_gen': Image.open('1.png').convert('RGB'),\n",
    "            # }\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor1o.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "x = processor1o(\n",
    "    text=[text],\n",
    "    images=[Image.open('1.png').convert('RGB')],\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "print(processor1o.batch_decode(x.input_ids, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3df39a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    }
   ],
   "source": [
    "from mini1o import Mini1oMLLM, Mini1oConfig\n",
    "import torch\n",
    "\n",
    "config = Mini1oConfig.from_pretrained('ckpt')\n",
    "\n",
    "model = Mini1oMLLM.from_config(config, \n",
    "                               torch_dtype=torch.bfloat16,\n",
    "                               use_flash_attn=False,\n",
    "                               trust_remote_code=True,\n",
    "                               device_map = 'auto').eval()\n",
    "model = model.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe97ca6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape mismatch: value tensor of shape [2304, 896] cannot be broadcast to indexing result of shape [9, 896]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:0\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, torch\u001b[38;5;241m.\u001b[39mTensor)}\n\u001b[0;32m      4\u001b[0m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m----> 5\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mx, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\29058\\Desktop\\minigpt-4o\\mini1o\\mllm.py:155\u001b[0m, in \u001b[0;36mMini1oMLLM.generate\u001b[1;34m(self, pixel_values, input_ids, attention_mask, visual_features, generation_config, output_hidden_states, **generate_kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m selected\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    154\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m未找到 image context token\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 155\u001b[0m     \u001b[43minput_embeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mselected\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m vit_embeds\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C)\u001b[38;5;241m.\u001b[39mto(input_embeds\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    156\u001b[0m     input_embeds \u001b[38;5;241m=\u001b[39m input_embeds\u001b[38;5;241m.\u001b[39mview(B, N, C)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape mismatch: value tensor of shape [2304, 896] cannot be broadcast to indexing result of shape [9, 896]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = {k: v.to('cuda:0') for k, v in x.items() if isinstance(v, torch.Tensor)}\n",
    "x['pixel_values'] = x['pixel_values'].to(torch.bfloat16)\n",
    "output = model.generate(**x, max_new_tokens=1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949d5fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
