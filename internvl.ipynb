{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ba7ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Sliding Window Attention is enabled but not implemented for `eager`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FlashAttention2 is not installed.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from mini1o.processor import Mini1oProcessor\n",
    "\n",
    "path = 'OpenGVLab/InternVL2_5-1B-MPO'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66fb4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "from mini1o.processor import Mini1oProcessor\n",
    "\n",
    "path = 'OpenGVLab/InternVL2_5-1B-MPO'\n",
    "model = AutoModel.from_pretrained(\n",
    "    path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    use_flash_attn=True,\n",
    "    trust_remote_code=True).eval().cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "processor = AutoProcessor.from_pretrained(path, trust_remote_code=True)\n",
    "processor1o = Mini1oProcessor.from_pretrained(path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bc21e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from decord import VideoReader, cpu\n",
    "from PIL import Image\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from transformers import AutoModel, AutoTokenizer, AutoProcessor\n",
    "\n",
    "path = 'OpenGVLab/InternVL2_5-1B-MPO'\n",
    "from mini1o.processor import Mini1oProcessor, Mini1oImageProcessor\n",
    "\n",
    "image_processor = Mini1oImageProcessor()\n",
    "tokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n",
    "chat_template = \"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}\\n{% if content['type'] == 'image_gen' or 'image_gen' in content %}<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\\n{% elif content['type'] == 'video_gen' or 'video_gen' in content %}<|video_gen_start|><|video_gen_pad|><|video_gen_end|>\\n{% elif content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>\\n{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\\n\"\n",
    "\n",
    "processor1o = Mini1oProcessor(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5686a8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor1o.save_pretrained('kirp' trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6511d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def build_transform(input_size):\n",
    "    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n",
    "    transform = T.Compose([\n",
    "        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n",
    "        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n",
    "    best_ratio_diff = float('inf')\n",
    "    best_ratio = (1, 1)\n",
    "    area = width * height\n",
    "    for ratio in target_ratios:\n",
    "        target_aspect_ratio = ratio[0] / ratio[1]\n",
    "        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n",
    "        if ratio_diff < best_ratio_diff:\n",
    "            best_ratio_diff = ratio_diff\n",
    "            best_ratio = ratio\n",
    "        elif ratio_diff == best_ratio_diff:\n",
    "            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n",
    "                best_ratio = ratio\n",
    "    return best_ratio\n",
    "\n",
    "def dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n",
    "    orig_width, orig_height = image.size\n",
    "    aspect_ratio = orig_width / orig_height\n",
    "\n",
    "    # calculate the existing image aspect ratio\n",
    "    target_ratios = set(\n",
    "        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n",
    "        i * j <= max_num and i * j >= min_num)\n",
    "    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n",
    "\n",
    "    # find the closest aspect ratio to the target\n",
    "    target_aspect_ratio = find_closest_aspect_ratio(\n",
    "        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n",
    "\n",
    "    # calculate the target width and height\n",
    "    target_width = image_size * target_aspect_ratio[0]\n",
    "    target_height = image_size * target_aspect_ratio[1]\n",
    "    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n",
    "\n",
    "    # resize the image\n",
    "    resized_img = image.resize((target_width, target_height))\n",
    "    processed_images = []\n",
    "    for i in range(blocks):\n",
    "        box = (\n",
    "            (i % (target_width // image_size)) * image_size,\n",
    "            (i // (target_width // image_size)) * image_size,\n",
    "            ((i % (target_width // image_size)) + 1) * image_size,\n",
    "            ((i // (target_width // image_size)) + 1) * image_size\n",
    "        )\n",
    "        # split the image\n",
    "        split_img = resized_img.crop(box)\n",
    "        processed_images.append(split_img)\n",
    "    assert len(processed_images) == blocks\n",
    "    if use_thumbnail and len(processed_images) != 1:\n",
    "        thumbnail_img = image.resize((image_size, image_size))\n",
    "        processed_images.append(thumbnail_img)\n",
    "    return processed_images\n",
    "\n",
    "def load_image(image_file, input_size=448, max_num=12):\n",
    "    image = Image.open(image_file).convert('RGB')\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "    pixel_values = [transform(image) for image in images]\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    return pixel_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d4ce384",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = load_image('1.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86bed699",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor1o = Mini1oProcessor.from_pretrained(path, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc0126f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image shows details of a HivePOT token with a balance of 0ETH. It includes information like ETH balance, transaction history, token tracking links, and a contract creator. There's a note indicating the token is reported to be a honeyypoten. The user interface includes options for buying, exchanging, and playing.\n"
     ]
    }
   ],
   "source": [
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "337e18ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from kirp\\mini4o\\chat_template.jinja\n",
    "with open('kirp\\mini4o\\chat_template.jinja', 'r') as f:\n",
    "    template = f.read()\n",
    "tokenizer.chat_template = template\n",
    "processor.tokenizer = tokenizer\n",
    "processor.chat_template = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec82c703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{% set image_count = namespace(value=0) %}{% set video_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}\\n{% if content['type'] == 'image_gen' or 'image_gen' in content %}<|image_gen_start|><|image_gen_pad|><|image_gen_end|>\\n{% elif content['type'] == 'video_gen' or 'video_gen' in content %}<|video_gen_start|><|video_gen_pad|><|video_gen_end|>\\n{% elif content['type'] == 'image' or 'image' in content or 'image_url' in content %}{% set image_count.value = image_count.value + 1 %}{% if add_vision_id %}Picture {{ image_count.value }}: {% endif %}<|vision_start|><|image_pad|><|vision_end|>\\n{% elif content['type'] == 'video' or 'video' in content %}{% set video_count.value = video_count.value + 1 %}{% if add_vision_id %}Video {{ video_count.value }}: {% endif %}<|vision_start|><|video_pad|><|vision_end|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8893f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor1o.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c11236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor1o.chat_template = template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1136078",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Compose is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# save processor1o to kirp/mini1o\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mprocessor1o\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkirp/mini1o\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\transformers\\processing_utils.py:619\u001b[0m, in \u001b[0;36mProcessorMixin.save_pretrained\u001b[1;34m(self, save_directory, push_to_hub, **kwargs)\u001b[0m\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(attribute, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_set_processor_class\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    618\u001b[0m         attribute\u001b[38;5;241m.\u001b[39m_set_processor_class(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 619\u001b[0m     \u001b[43mattribute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_directory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_auto_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;66;03m# We added an attribute to the init_kwargs of the tokenizers, which needs to be cleaned up.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attribute_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattributes:\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\transformers\\image_processing_base.py:259\u001b[0m, in \u001b[0;36mImageProcessingMixin.save_pretrained\u001b[1;34m(self, save_directory, push_to_hub, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;66;03m# If we save using the predefined names, we can load using `from_pretrained`\u001b[39;00m\n\u001b[0;32m    257\u001b[0m output_image_processor_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_directory, IMAGE_PROCESSOR_NAME)\n\u001b[1;32m--> 259\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_image_processor_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    260\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage processor saved in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_image_processor_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m push_to_hub:\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\transformers\\image_processing_base.py:500\u001b[0m, in \u001b[0;36mImageProcessingMixin.to_json_file\u001b[1;34m(self, json_file_path)\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03mSave this instance to a JSON file.\u001b[39;00m\n\u001b[0;32m    494\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;124;03m        Path to the JSON file in which this image_processor instance's parameters will be saved.\u001b[39;00m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(json_file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m--> 500\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_json_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\transformers\\image_processing_base.py:489\u001b[0m, in \u001b[0;36mImageProcessingMixin.to_json_string\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _processor_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    487\u001b[0m     dictionary[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessor_class\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _processor_class\n\u001b[1;32m--> 489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\__init__.py:238\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m--> 238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Compose is not JSON serializable"
     ]
    }
   ],
   "source": [
    "# save processor1o to kirp/mini1o\n",
    "processor1o.save_pretrained('kirp/mini1o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e579360",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6dbd7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \"image\": Image.open('1.png').convert('RGB'),\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\", \n",
    "                \"text\": \"Please describe the image shortly.\\n\"\n",
    "            },\n",
    "            # {\n",
    "            #     'type': 'image_gen',\n",
    "            #     'image_gen': Image.open('1.png').convert('RGB'),\n",
    "            # }\n",
    "        ],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0ac4a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>\n",
      "Please describe the image shortly.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = processor1o.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "daf3a0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = processor1o.image_processor([Image.open('1.png').convert('RGB')], return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6528ddf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = processor1o(\n",
    "    text=[text],\n",
    "    images=[Image.open('1.png').convert('RGB')]*2,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ad8486",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[151644,   8948,    198,   2610,    525,    264,  10950,  17847,     13,\n",
       "         151645,    198, 151644,    872,    198, 151652, 151655, 151655, 151655,\n",
       "         151655, 151655, 151655, 151655, 151655, 151655, 151653,    198,   5501,\n",
       "           7512,    279,   2168,  19620,    624, 151645,    198, 151644,  77091,\n",
       "            198]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b969f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|image_pad|><|vision_end|>\n",
      "Please describe the image shortly.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(processor1o.batch_decode(x.input_ids, skip_special_tokens=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d965c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47a244aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, who are you?\n",
      "Assistant: Hello! I'm an AI assistant whose name is InternVL, developed jointly by Shanghai AI Lab, Tsinghua University and other partners.\n",
      "User: Can you tell me a story?\n",
      "Assistant: Sure! Here's a story for you:\n",
      "\n",
      "In a distant galaxy bound for the distant corners of the Milky Way, there lived a planet called Terra. Terra was a lush, vibrant world where animals roamed freely, trees swayed in the wind, and rivers flowed with crystal clear water. The inhabitants of Terra, known as the Terrae, had their own unique language, rhythms, and customs.\n",
      "\n",
      "One day, under the watchful eye of an ancient Lumina, a force that could alter the course of the universe itself, a great cosmic event occurred. A massive black hole was detected in the distant center of the galaxy, and its gravitational pull began to warp space-time. This, in turn, disrupted the flow of energy that powered Terra.\n",
      "\n",
      "Without a fixed source of power, the Terrae were plunged into chaos. The once-universe-rotating landscapes twisted unnaturally, causing animals to scramble in fear for their homes. Rivers that had once flowed gracefully died, and rivers that once flowed steadily died. Buildings trembled, their timbers bending under the weight of the shifting landscape.\n",
      "\n",
      "As the Terrae struggled with this cosmic ordeal, they reached out to Earth, hoping that someone might know of a way to harness the immense power of their planet to counteract the black hole. Earth sent out a global call for unity and offered assistance, but no one else had this knowledge.\n",
      "\n",
      "With great patience, the Terrae devised a plan that would require all of their minds and efforts. They formed a team that would work together, dividing tasks among each of their inhabitants. Within days, the Terrae devised a plan to send a spacecraft to the black hole in the other direction. With this mission, the Terrae hoped to find a way to stop the black hole and regain control over the universe.\n",
      "\n",
      "Many were worried the spacecraft would fail, but what began as chaos ended in a moment of unity. The spacecraft successfully broke free of space and space-time, piloted by the willing Terrae members of the spaceship. They embarked on their perilous journey, driven by the trust in their planet's might.\n",
      "\n",
      "In the darkness of space, the ship traveled to the black hole, where it was finally able to harness the power of Terra's energies. With their cooperation, they sent a signal to the black hole, and after what seemed like an eternity, the gravitational attraction ceased.\n",
      "\n",
      "Panic erupted among the Terrae as they realized that they had brought a change from the cosmos. The black hole, sensing no resistance, allowed the Terrae to traverse space effortlessly. The change was not just a matter of a black hole being stopped; it was a real transformation, bringing stability and a new perspective to Earth.\n",
      "\n",
      "The Terrae had proven not just the power of their planet, but the strength of their unity. Terra, freed from the cosmic chaos, had become a symbol of hope in a universe teeming with mysteries and potential. And so, the Terrae, working together as a single entity, continued their journey through space, inspiring others to seek the strength found in unity.\n",
      "\n",
      "And from that day forward, Terra's impact on the universe went far beyond its physical boundaries. It became a testament to the boundless potential of life on Earth and the transformative power of unity.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image shortly.\n",
      "Assistant: The image shows a web page from an online exchange displaying a trading contract. It is part of a giveaway for \"Rainbet Casino.\" It details an Ethereum (ETH) contract with a balance of 0 ETH. There's discussion about the contract, including an entry for participation. Important data includes an approval status, transaction history, and network tags like \"Bella.\" There's a mention of a honeyypot token on the overview. Tabs for internal transactions, events, analytics, etc., are visible, but no actual transactions are listed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: <image>\n",
      "Please describe the image in detail.\n",
      "Assistant: This image appears to be a screenshot from a contract interface for a cryptocurrency or blockchain trading platform. Here's a detailed breakdown of the features and contents:\n",
      "\n",
      "1. **Header Information**:\n",
      "   - **Contract ID**: 0x34C6211621f2763c6E0Eb007dc2aE9E1090A2d22f6\n",
      "   - **Sponsored Event**: Rainbet Casino - 20K USD WEEKLY RAFFLE, promoting a free entry to claim a game ticket.\n",
      "   - **Options Tabs**: Buy, Exchange, Play, Gaming\n",
      "\n",
      "2. **Token Details**:\n",
      "   - **Token Type**: Honeypont Token, specifically labeled as \"Belle\".\n",
      "   - **Source Code**: Tap on a button to get more details.\n",
      "   - **Overview**:\n",
      "     - **ETH Balance**: 0 ETH\n",
      "     - **ETH Value**: $0.00\n",
      "     - **Token Holdings**: 0 (1 Tokens)\n",
      "   - **More Info**:\n",
      "     - Private Name Tags: \"+ Add\", indicating the contract can be associated with multiple name tags.\n",
      "     - Contract Creator Name: BELLE (Honeypont Rug P)\n",
      "     - ERC-20 Token Identifier: BTC-20 (associated with BELLE)\n",
      "\n",
      "3. **Transaction Details**:\n",
      "   - **Last 25 Transactions**: Shows a large amount of recent events, primarily within the \"Recent\" tab.\n",
      "   - **Transaction Hashes, Methods, Blocks, Age, Block Number, From, To, Amount, Txn Fee**:\n",
      "     - **Transactions**:\n",
      "       - 0xe9ec1d4a257c6df5f9c2f594, Approved, 21519440 blocks, 103 days ago, from \"0x2C6B2D...e76Cf5s294\"\n",
      "       - 0x7a41584abe5, Approved, 19903137 blocks, 328 days ago, from \"0xe64febe9...147E37403\"\n",
      "       - 0xd1b3d045a, Approve, 18719069 blocks, 494 days ago, from \"0xe64febe9...17453043\"\n",
      "       - More transactions (additional blocks and associated Eth transfers)\n",
      "\n",
      "4. **Additional Elements**:\n",
      "   - **Promotional Banner**: An advertisement for CryptoCasing & Sportsbook titled \"Jack Bitcoin & Golf Bingo\".\n",
      "   - **Multichain Info**: Displays a $0 (Multichain Portfolio), indicating no addresses found for the contract’s multiservice identifier.\n",
      "   - **Filters/Advanced Options**: Options to download page data and filter transactions.\n",
      "\n",
      "The overall layout is clean, with tabs and lists providing various features relevant to viewing and managing contracts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151645 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Please write a poem according to the image.\n",
      "Assistant: In the tapestry of blockchain dreams,\n",
      "A token whispers with a gentle sway,\n",
      "Bello Gentle from BELLE, rare within.\n",
      "A token’s hush in cryptic, silent form,\n",
      "A token-holding tale born in May.\n",
      "\n",
      "An ethereal hash didst enter its land,\n",
      "“Approval” in its heart, a token’s embrace.\n",
      "Tokens’ tales unbound, in a digital tide,\n",
      "A tale unfurled beneath lines, in token streams.\n",
      "\n",
      "With blocks stately, in lines they play,\n",
      "From honesies in an art of unyielding grace.\n",
      "Each block, a token’s latest revelation,\n",
      "In tweets and fees, a token’s silent sigh.\n",
      "\n",
      "In the heart of exchanges and in gaming's might,\n",
      "BELLE hails from the Honeypont Forge.\n",
      "A token tokenized promise, in chains entwined,\n",
      "In tokens of Ether, a token’s eternal keep.\n",
      "\n",
      "A network of footings, intertwined and free,\n",
      "The token soars with a rhythm divine.\n",
      "With every block it writes, in the game of the blockchain wide,\n",
      "Bello Gentle holds the token, in its ethereal way.\n",
      "\n",
      "Honey, tokens, token-holding dream,\n",
      "A token tale unfolds, in blockchain light.\n",
      "In exchanges and in dreams, in tokens so dear,\n",
      "BELLE, in this tale, a token’s best kept share.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './examples/image1.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 121\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAssistant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# multi-image multi-round conversation, combined images (多图多轮对话，拼接图像)\u001b[39;00m\n\u001b[1;32m--> 121\u001b[0m pixel_values1 \u001b[38;5;241m=\u001b[39m \u001b[43mload_image\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./examples/image1.jpg\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_num\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    122\u001b[0m pixel_values2 \u001b[38;5;241m=\u001b[39m load_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./examples/image2.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, max_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mbfloat16)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    123\u001b[0m pixel_values \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pixel_values1, pixel_values2), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 76\u001b[0m, in \u001b[0;36mload_image\u001b[1;34m(image_file, input_size, max_num)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_image\u001b[39m(image_file, input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m448\u001b[39m, max_num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_file\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     77\u001b[0m     transform \u001b[38;5;241m=\u001b[39m build_transform(input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[0;32m     78\u001b[0m     images \u001b[38;5;241m=\u001b[39m dynamic_preprocess(image, image_size\u001b[38;5;241m=\u001b[39minput_size, use_thumbnail\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_num\u001b[38;5;241m=\u001b[39mmax_num)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\PIL\\Image.py:3247\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3244\u001b[0m     filename \u001b[38;5;241m=\u001b[39m fp\n\u001b[0;32m   3246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filename:\n\u001b[1;32m-> 3247\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3248\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './examples/image1.jpg'"
     ]
    }
   ],
   "source": [
    "# set the max number of tiles in `max_num`\n",
    "pixel_values = load_image('1.png', max_num=12).to(torch.bfloat16).cuda()\n",
    "generation_config = dict(max_new_tokens=1024, do_sample=True)\n",
    "\n",
    "# pure-text conversation (纯文本对话)\n",
    "question = 'Hello, who are you?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Can you tell me a story?'\n",
    "response, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image single-round conversation (单图单轮对话)\n",
    "question = '<image>\\nPlease describe the image shortly.'\n",
    "response = model.chat(tokenizer, pixel_values, question, generation_config)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# single-image multi-round conversation (单图多轮对话)\n",
    "question = '<image>\\nPlease describe the image in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Please write a poem according to the image.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, combined images (多图多轮对话，拼接图像)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "question = '<image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# multi-image multi-round conversation, separate images (多图多轮对话，独立图像)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "\n",
    "question = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'What are the similarities and differences between these two images.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list,\n",
    "                               history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# batch inference, single image per sample (单图批处理)\n",
    "pixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "pixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\n",
    "num_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n",
    "pixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n",
    "\n",
    "questions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\n",
    "responses = model.batch_chat(tokenizer, pixel_values,\n",
    "                             num_patches_list=num_patches_list,\n",
    "                             questions=questions,\n",
    "                             generation_config=generation_config)\n",
    "for question, response in zip(questions, responses):\n",
    "    print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "# video multi-round conversation (视频多轮对话)\n",
    "def get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n",
    "    if bound:\n",
    "        start, end = bound[0], bound[1]\n",
    "    else:\n",
    "        start, end = -100000, 100000\n",
    "    start_idx = max(first_idx, round(start * fps))\n",
    "    end_idx = min(round(end * fps), max_frame)\n",
    "    seg_size = float(end_idx - start_idx) / num_segments\n",
    "    frame_indices = np.array([\n",
    "        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n",
    "        for idx in range(num_segments)\n",
    "    ])\n",
    "    return frame_indices\n",
    "\n",
    "def load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n",
    "    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n",
    "    max_frame = len(vr) - 1\n",
    "    fps = float(vr.get_avg_fps())\n",
    "\n",
    "    pixel_values_list, num_patches_list = [], []\n",
    "    transform = build_transform(input_size=input_size)\n",
    "    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n",
    "    for frame_index in frame_indices:\n",
    "        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n",
    "        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n",
    "        pixel_values = [transform(tile) for tile in img]\n",
    "        pixel_values = torch.stack(pixel_values)\n",
    "        num_patches_list.append(pixel_values.shape[0])\n",
    "        pixel_values_list.append(pixel_values)\n",
    "    pixel_values = torch.cat(pixel_values_list)\n",
    "    return pixel_values, num_patches_list\n",
    "\n",
    "video_path = './examples/red-panda.mp4'\n",
    "pixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\n",
    "pixel_values = pixel_values.to(torch.bfloat16).cuda()\n",
    "video_prefix = ''.join([f'Frame{i+1}: <image>\\n' for i in range(len(num_patches_list))])\n",
    "question = video_prefix + 'What is the red panda doing?'\n",
    "# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=None, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n",
    "\n",
    "question = 'Describe this video in detail.'\n",
    "response, history = model.chat(tokenizer, pixel_values, question, generation_config,\n",
    "                               num_patches_list=num_patches_list, history=history, return_history=True)\n",
    "print(f'User: {question}\\nAssistant: {response}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "765e5c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.6.0+cu124 with CUDA 1204 (you have 2.5.1+cu121)\n",
      "    Python  3.10.11 (you have 3.10.16)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "A matching Triton is not available, some optimizations will not be enabled\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\xformers\\__init__.py\", line 57, in _is_triton_available\n",
      "    import triton  # noqa\n",
      "ModuleNotFoundError: No module named 'triton'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc923d4ce94343adbe82dd3f9f349a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Error no file named config.json found in directory C:\\Users\\29058\\.cache\\huggingface\\hub\\models--Efficient-Large-Model--Sana_Sprint_0.6B_1024px_diffusers\\snapshots\\a7d9fc31dd5c3f5e22dbfd78360777ceed56ae97\\vae.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdiffusers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SanaSprintPipeline\n\u001b[1;32m----> 4\u001b[0m pipe \u001b[38;5;241m=\u001b[39m \u001b[43mSanaSprintPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEfficient-Large-Model/Sana_Sprint_0.6B_1024px_diffusers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m pipe\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m image \u001b[38;5;241m=\u001b[39m pipe(prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma tiny astronaut hatching from an egg on the moon\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:961\u001b[0m, in \u001b[0;36mDiffusionPipeline.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    955\u001b[0m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[0;32m    956\u001b[0m     sub_model_dtype \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    957\u001b[0m         torch_dtype\u001b[38;5;241m.\u001b[39mget(name, torch_dtype\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39mfloat32))\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[0;32m    960\u001b[0m     )\n\u001b[1;32m--> 961\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    985\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n\u001b[0;32m    989\u001b[0m init_kwargs[name] \u001b[38;5;241m=\u001b[39m loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\diffusers\\pipelines\\pipeline_loading_utils.py:777\u001b[0m, in \u001b[0;36mload_sub_model\u001b[1;34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options)\u001b[0m\n\u001b[0;32m    775\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name)):\n\u001b[1;32m--> 777\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cached_folder, name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# else load from the root directory\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     loaded_sub_model \u001b[38;5;241m=\u001b[39m load_method(cached_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mloading_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\diffusers\\models\\modeling_utils.py:984\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    981\u001b[0m config_path \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    983\u001b[0m \u001b[38;5;66;03m# load config\u001b[39;00m\n\u001b[1;32m--> 984\u001b[0m config, unused_kwargs, commit_hash \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_config(\n\u001b[0;32m    985\u001b[0m     config_path,\n\u001b[0;32m    986\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m    987\u001b[0m     return_unused_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    988\u001b[0m     return_commit_hash\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    989\u001b[0m     force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[0;32m    990\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m    991\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m    992\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m    993\u001b[0m     revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[0;32m    994\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[0;32m    995\u001b[0m     user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[0;32m    996\u001b[0m     dduf_entries\u001b[38;5;241m=\u001b[39mdduf_entries,\n\u001b[0;32m    997\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    998\u001b[0m )\n\u001b[0;32m    999\u001b[0m \u001b[38;5;66;03m# no in-place modification of the original config.\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m config \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(config)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\29058\\miniconda3\\envs\\hi\\lib\\site-packages\\diffusers\\configuration_utils.py:384\u001b[0m, in \u001b[0;36mConfigMixin.load_config\u001b[1;34m(cls, pretrained_model_name_or_path, return_unused_kwargs, return_commit_hash, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pretrained_model_name_or_path, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    385\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m found in directory \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m         )\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: Error no file named config.json found in directory C:\\Users\\29058\\.cache\\huggingface\\hub\\models--Efficient-Large-Model--Sana_Sprint_0.6B_1024px_diffusers\\snapshots\\a7d9fc31dd5c3f5e22dbfd78360777ceed56ae97\\vae."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import SanaSprintPipeline\n",
    "\n",
    "pipe = SanaSprintPipeline.from_pretrained(\n",
    "    \"Efficient-Large-Model/Sana_Sprint_0.6B_1024px_diffusers\", torch_dtype=torch.bfloat16\n",
    ")\n",
    "pipe.to(\"cuda\")\n",
    "\n",
    "image = pipe(prompt=\"a tiny astronaut hatching from an egg on the moon\")[0]\n",
    "image[0].save(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41551f45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
